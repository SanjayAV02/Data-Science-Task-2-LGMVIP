# -*- coding: utf-8 -*-
"""Stock_Market_Prediction_And_Forecasting_Using_Stacked_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rkC1wrC6ZGHbMBEkZ5zRETlSYIXtqUaF

# LetsGrowMore (LGMVIP) - May 2022
"""



"""Sanjay A V - Data Science Intern

## BEGINNER LEVEL TASK

## Task 2 - Stock Market Prediction And Forecasting Using Stacked LSTM

## 1. IMPORTING THE NECESSARY LIBRARIES
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

"""## 2. IMPORTING THE DATASET"""

url = 'https://raw.githubusercontent.com/mwitiderrick/stockprice/master/NSE-TATAGLOBAL.csv'
data = pd.read_csv(url)

"""## 3. ANALYSIS"""

data.head(10)

data.tail(5)

data.shape

data.columns

data.info()

data.isnull().sum()

data.describe()

"""#### PICKING UP CLOSE COLUMN"""

data1 = data.reset_index()['Close']

data1

data1.shape

"""#### VISUALIZING"""

plt.plot(data1)
plt.title("Stacked Index View")
plt.xlabel("X-axis")
plt.ylabel("Y-axis")

"""### MIN-MAX SCALER

LSTM are sensitive to the scale of the data, so using Min-Max scaler to transform the values from 0 to 1. So in order to do that we need to reshape so that we can fit transform
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range = (0,1))
data1 = scaler.fit_transform(np.array(data1).reshape(-1,1))

"""Now the data1 is transformed into an array and it now having the values between zero to one

"""

data1

data1.shape

"""### TRAIN TEST SPLIT

Since we are going to train the Time series data so one data is dependent on other data therefore the training size should be 65% of the total length of the data frame whereas the test size should be the difference between the length of the dataset and the training size
"""

train_size = int(len(data1)*0.65)
test_size = len(data1) - train_size
train_data, test_data = data1[0:train_size,:],data1[train_size:len(data1),:1]

train_size,test_size

train_data,test_data

"""### DATA PREPROCESSING

Now, Splitting the data into x and y, the timestep value will be 100. In the 0th iteration the first 100 records will goes as first record and the 101 elements will be in the x. The 100 elements will be in the y

converting an array of values into a dataset matrix
"""

def create_data(dataset,time_step =  1):
    dataX, dataY = [], []
    for i in range(len(dataset) - time_step - 1):
        a = dataset[i:(i + time_step), 0]
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return np.array(dataX), np.array(dataY)

time_step  = 100
X_train, y_train = create_data(train_data,time_step)
X_test,y_test = create_data(test_data,time_step)

X_train

X_train.shape,y_train.shape

X_test.shape,y_test.shape

"""Before implementing the LSTM, we should always reshape our X_train in 3D and add 1, the reason behind it is the time step and the 1 is given to the LSTM"""

X_train = X_train.reshape(X_train.shape[0],X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)

"""### STACKED LSTM"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM

model = Sequential()
model.add(LSTM(50, return_sequences = True, input_shape = (100,1)))
model.add(LSTM(50, return_sequences = True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss = 'mean_squared_error', optimizer = 'adam')

model.summary()

model.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 100, batch_size = 64, verbose = 1)

"""### PREDICTION

Now predicting the x_train and y_train, and in order to see the root mean squared performance lets scaler inverse transform
"""

train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)

"""Calculating root_mean_square performance metrics"""

import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))

"""Testing data root_mean_square"""

math.sqrt(mean_squared_error(y_test,test_predict))

"""### PLOTTING"""

look_back = 100

# shift train prediction 

trainPredict = np.empty_like(data1)
trainPredict[:,:] = np.nan
trainPredict[look_back:len(train_predict)+look_back, :] = train_predict

# shift test prediction

testPredict = np.empty_like(data1)
testPredict[:,:] = np.nan
testPredict[len(train_predict) + (look_back * 2)+1:len(data1)-1, :] = test_predict

# plot baseline and predictions

plt.plot(scaler.inverse_transform(data1))
plt.plot(trainPredict)
plt.plot(testPredict)
plt.show()